{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "from numpy import ma\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3020: DtypeWarning: Columns (2,3,4,5,6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "names =['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "df = pd.read_csv('train.csv',header=None,sep=\",\",names=names, encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['toxic_score'] = df[['toxic','severe_toxic','obscene', 'threat', 'insult', 'identity_hate']].max(axis=1)\n",
    "#drop unnecessary columns\n",
    "df = df.drop(['id', 'toxic','severe_toxic','obscene', 'threat', 'insult', 'identity_hate'], axis=1)\n",
    "#drop first row\n",
    "df.drop(df.index[:1], inplace=True)\n",
    "#df.drop(df.index[:150000], inplace=True) #smaller data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text toxic_score\n",
       "1  Explanation\\nWhy the edits made under my usern...           0\n",
       "2  D'aww! He matches this background colour I'm s...           0\n",
       "3  Hey man, I'm really not trying to edit war. It...           0\n",
       "4  \"\\nMore\\nI can't make any real suggestions on ...           0\n",
       "5  You, sir, are my hero. Any chance you remember...           0"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_words = []\n",
    "negative_file = open(\"negative-words.txt\", \"r\")\n",
    "for line in negative_file:\n",
    "    bad_words.append(line.strip(\"\\n\"))\n",
    "negative_file.close()\n",
    "profanity_file = open(\"profanity-words.txt\", \"r\")\n",
    "for line in profanity_file:\n",
    "    if (line.strip(\"\\n\") not in bad_words):\n",
    "        bad_words.append(line.strip(\"\\n\"))\n",
    "profanity_file.close()\n",
    "# print(bad_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "data = df.values\n",
    "\n",
    "X = data[:,0].reshape((159571,1))\n",
    "Y = data[:,1].reshape((159571,1))\n",
    "\n",
    "toxic_count = 0\n",
    "not_toxic_count = 0\n",
    "\n",
    "X_tr = []\n",
    "Y_tr = []\n",
    "\n",
    "for index in range(0,159571):\n",
    "    if (int(Y[index][0]) == 1 and toxic_count < 7500) or (int(Y[index][0]) == 0 and not_toxic_count < 7500):\n",
    "        Y_tr.append(int(Y[index][0]))\n",
    "        X_tr.append(X[index][0])\n",
    "        if (int(Y[index][0]) == 1):\n",
    "            toxic_count += 1\n",
    "        else:\n",
    "            not_toxic_count += 1\n",
    "    if toxic_count == 7500 and not_toxic_count == 7500:\n",
    "        break\n",
    "\n",
    "# X_tr = np.array(X_tr).reshape((15000, 1))\n",
    "Y_tr = np.array(Y_tr).reshape((15000, 1))\n",
    "\n",
    "cv = sklearn.feature_extraction.text.CountVectorizer(vocabulary=bad_words)\n",
    "X_tr_features = cv.fit_transform(X_tr).toarray()\n",
    "\n",
    "print(X_tr_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling \n",
    "# X_tr_scale = preprocessing.scale(X_tr_features)\n",
    "X_tr_scale = X_tr_features + 0.000001\n",
    "# X_tr_scale = preprocessing.scale(X_tr_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.e+00 1.e-06 1.e-06 ... 1.e-06 1.e-06 1.e-06]\n",
      " [1.e+00 1.e-06 1.e-06 ... 1.e-06 1.e-06 1.e-06]\n",
      " [1.e+00 1.e-06 1.e-06 ... 1.e-06 1.e-06 1.e-06]\n",
      " ...\n",
      " [1.e+00 1.e-06 1.e-06 ... 1.e-06 1.e-06 1.e-06]\n",
      " [1.e+00 1.e-06 1.e-06 ... 1.e-06 1.e-06 1.e-06]\n",
      " [1.e+00 1.e-06 1.e-06 ... 1.e-06 1.e-06 1.e-06]]\n"
     ]
    }
   ],
   "source": [
    "# Append a column of ones in the beginning of x_train an save in variable a.\n",
    "ones = np.ones(X_tr_scale.shape[0]).reshape((X_tr_scale.shape[0], 1))\n",
    "a = np.hstack((ones, X_tr_scale))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5214, 1)\n"
     ]
    }
   ],
   "source": [
    "w = np.zeros((a.shape[1], 1))\n",
    "print(w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypothesis(a , w):\n",
    "    return sigmoid(np.dot(a, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " ...\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]]\n"
     ]
    }
   ],
   "source": [
    "yhat = hypothesis(a, w)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-10397.207708399179\n"
     ]
    }
   ],
   "source": [
    "def likelihood(X_tr , Y_tr , w , n):\n",
    "    yhat = hypothesis(X_tr, w)\n",
    "    Y_tr = Y_tr.reshape((yhat.shape[0], 1))\n",
    "    likelihood = np.sum(Y_tr * ma.log(yhat) + (1 - Y_tr) * ma.log(1 - yhat))\n",
    "    return likelihood\n",
    "\n",
    "print(likelihood(a, Y_tr, w, a.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "13600\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "14000\n",
      "14100\n",
      "14200\n",
      "14300\n",
      "14400\n",
      "14500\n",
      "14600\n",
      "14700\n",
      "14800\n",
      "14900\n",
      "15000\n",
      "15100\n",
      "15200\n",
      "15300\n",
      "15400\n",
      "15500\n",
      "15600\n",
      "15700\n",
      "15800\n",
      "15900\n",
      "16000\n",
      "16100\n",
      "16200\n",
      "16300\n",
      "16400\n",
      "16500\n",
      "16600\n",
      "16700\n",
      "16800\n",
      "16900\n",
      "17000\n",
      "17100\n",
      "17200\n",
      "17300\n",
      "17400\n",
      "17500\n",
      "17600\n",
      "17700\n",
      "17800\n",
      "17900\n",
      "18000\n",
      "18100\n",
      "18200\n",
      "18300\n",
      "18400\n",
      "18500\n",
      "18600\n",
      "18700\n",
      "18800\n",
      "18900\n",
      "19000\n",
      "19100\n",
      "19200\n",
      "19300\n",
      "19400\n",
      "19500\n",
      "19600\n",
      "19700\n",
      "19800\n",
      "19900\n",
      "20000\n",
      "20100\n",
      "20200\n",
      "20300\n",
      "20400\n",
      "20500\n",
      "20600\n",
      "20700\n",
      "20800\n",
      "20900\n",
      "21000\n",
      "21100\n",
      "21200\n",
      "21300\n",
      "21400\n",
      "21500\n",
      "21600\n",
      "21700\n",
      "21800\n",
      "21900\n",
      "22000\n",
      "22100\n",
      "22200\n",
      "22300\n",
      "22400\n",
      "22500\n",
      "22600\n",
      "22700\n",
      "22800\n",
      "22900\n",
      "23000\n",
      "23100\n",
      "23200\n",
      "23300\n",
      "23400\n",
      "23500\n",
      "23600\n",
      "23700\n",
      "23800\n",
      "23900\n",
      "24000\n",
      "24100\n",
      "24200\n",
      "24300\n",
      "24400\n",
      "24500\n",
      "24600\n",
      "24700\n",
      "24800\n",
      "24900\n",
      "25000\n",
      "25100\n",
      "25200\n",
      "25300\n",
      "25400\n",
      "25500\n",
      "25600\n",
      "25700\n",
      "25800\n",
      "25900\n",
      "26000\n",
      "26100\n",
      "26200\n",
      "26300\n",
      "26400\n",
      "26500\n",
      "26600\n",
      "26700\n",
      "26800\n",
      "26900\n",
      "27000\n",
      "27100\n",
      "27200\n",
      "27300\n",
      "27400\n",
      "27500\n",
      "27600\n",
      "27700\n",
      "27800\n",
      "27900\n",
      "28000\n",
      "28100\n",
      "28200\n",
      "28300\n",
      "28400\n",
      "28500\n",
      "28600\n",
      "28700\n",
      "28800\n",
      "28900\n",
      "29000\n",
      "29100\n",
      "29200\n",
      "29300\n",
      "29400\n",
      "29500\n",
      "29600\n",
      "29700\n",
      "29800\n",
      "29900\n",
      "30000\n",
      "30100\n",
      "30200\n",
      "30300\n",
      "30400\n",
      "30500\n",
      "30600\n",
      "30700\n",
      "30800\n",
      "30900\n",
      "31000\n",
      "31100\n",
      "31200\n",
      "31300\n",
      "31400\n",
      "31500\n",
      "31600\n",
      "31700\n",
      "31800\n",
      "31900\n",
      "32000\n",
      "32100\n",
      "32200\n",
      "32300\n",
      "32400\n",
      "32500\n",
      "32600\n",
      "32700\n",
      "32800\n",
      "32900\n",
      "33000\n",
      "33100\n",
      "33200\n",
      "33300\n",
      "33400\n",
      "33500\n",
      "33600\n",
      "33700\n",
      "33800\n",
      "33900\n",
      "34000\n",
      "34100\n",
      "34200\n",
      "34300\n",
      "34400\n",
      "34500\n",
      "34600\n",
      "34700\n",
      "34800\n",
      "34900\n",
      "35000\n",
      "35100\n",
      "35200\n",
      "35300\n",
      "35400\n",
      "35500\n",
      "35600\n",
      "35700\n",
      "35800\n",
      "35900\n",
      "36000\n",
      "36100\n",
      "36200\n",
      "36300\n",
      "36400\n",
      "36500\n",
      "36600\n",
      "36700\n",
      "36800\n",
      "36900\n",
      "37000\n",
      "37100\n",
      "37200\n",
      "37300\n",
      "37400\n",
      "37500\n",
      "37600\n",
      "37700\n",
      "37800\n",
      "37900\n",
      "38000\n",
      "38100\n",
      "38200\n",
      "38300\n",
      "38400\n",
      "38500\n",
      "38600\n",
      "38700\n",
      "38800\n",
      "38900\n",
      "39000\n",
      "39100\n",
      "39200\n",
      "39300\n",
      "39400\n",
      "39500\n",
      "39600\n",
      "39700\n",
      "39800\n",
      "39900\n",
      "40000\n",
      "40100\n",
      "40200\n",
      "40300\n",
      "40400\n",
      "40500\n",
      "40600\n",
      "40700\n",
      "40800\n",
      "40900\n",
      "41000\n",
      "41100\n",
      "41200\n",
      "41300\n",
      "41400\n",
      "41500\n",
      "41600\n",
      "41700\n",
      "41800\n",
      "41900\n",
      "42000\n",
      "42100\n",
      "42200\n",
      "42300\n",
      "42400\n",
      "42500\n",
      "42600\n",
      "42700\n",
      "42800\n",
      "42900\n",
      "43000\n",
      "43100\n",
      "43200\n",
      "43300\n",
      "43400\n",
      "43500\n",
      "43600\n",
      "43700\n",
      "43800\n",
      "43900\n",
      "44000\n",
      "44100\n",
      "44200\n",
      "44300\n",
      "44400\n",
      "44500\n",
      "44600\n",
      "44700\n",
      "44800\n",
      "44900\n",
      "45000\n",
      "45100\n",
      "45200\n",
      "45300\n",
      "45400\n",
      "45500\n",
      "45600\n",
      "45700\n",
      "45800\n",
      "45900\n",
      "46000\n",
      "46100\n",
      "46200\n",
      "46300\n",
      "46400\n",
      "46500\n",
      "46600\n",
      "46700\n",
      "46800\n",
      "46900\n",
      "47000\n",
      "47100\n",
      "47200\n",
      "47300\n",
      "47400\n",
      "47500\n",
      "47600\n",
      "47700\n",
      "47800\n",
      "47900\n",
      "48000\n",
      "48100\n",
      "48200\n",
      "48300\n",
      "48400\n",
      "48500\n",
      "48600\n",
      "48700\n",
      "48800\n",
      "48900\n",
      "49000\n",
      "49100\n",
      "49200\n",
      "49300\n",
      "49400\n",
      "49500\n",
      "49600\n",
      "49700\n",
      "49800\n",
      "49900\n",
      "[[-1.05343942e+00]\n",
      " [-1.05343942e-06]\n",
      " [-1.05343942e-06]\n",
      " ...\n",
      " [ 1.34245081e-01]\n",
      " [-1.05343942e-06]\n",
      " [ 4.76864319e-02]]\n"
     ]
    }
   ],
   "source": [
    "# TODO - Write the gradient ascent function - 25 points\n",
    "def Gradient_Ascent(a, y, learning_rate, num_iters):\n",
    "    n = a.shape[0] # Number of training examples.\n",
    "    # TODO - Initialize w. Zeros vector of shape x_train.shape[1],1\n",
    "    w = np.zeros((a.shape[1], 1))\n",
    "    # TODO - Reshape y to be a rank 2 matrix.\n",
    "    y = y.reshape((y.shape[0], 1))\n",
    "    # TODO - Initiating list to store values of likelihood after few iterations.\n",
    "    likelihood_values = []\n",
    "    for i in range(num_iters):\n",
    "        yhat = hypothesis(a, w)\n",
    "        error = y - yhat\n",
    "        gradient = np.dot(a.T, error)\n",
    "        # Updating Parameters\n",
    "        w = w + (learning_rate / n) * gradient\n",
    "        if (i % 100) == 0:\n",
    "            print(i)\n",
    "            likelihood_values.append(likelihood(a,y,w,n))\n",
    "        \n",
    "    return w, likelihood_values\n",
    "\n",
    "learning_rate = 0.1\n",
    "num_iters = 50000\n",
    "w, likelihood_values = Gradient_Ascent(a, Y_tr, learning_rate, num_iters)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5102.653630073383\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ0AAAEWCAYAAAC9qEq5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3XmcXFWd9/HPtzsbBJIIwUQgEEBQg8oWgYBLAxGCoDCu4AIII7I5+qgT4NHHURRGcBRwVJjIPqIhAypMRJElhYLNFhYhIBBCoAOEELJACOmkO7/nj3s6VBfVa2rrru87r3r1vecudc6tSv3qLHWuIgIzM7NKaKh2BszMrH446JiZWcU46JiZWcU46JiZWcU46JiZWcU46JiZWcU46FiPJH1A0uN56wslTe3Heb4r6VdpeTtJqyQ1pvWcpH8uXa67zMNxku4o9/NUUv51rdLz/0DSUkmLq5WH7kj6nKQ/VzsflnHQsQ26CiYR8deIeEcpnysino2IzSKivZTnrQUpgK6RNCEvbaqkhVXMVllI2g74BjApIsYX2d4kaVHeelm/XEiaKCkkDelIi4irI+Lgcj2n9Y2Djll5vAb8v2pnoq/yP6x7aTvg5YhYUo78FOqoGdvA5aBjPSr8tlqw7V2SnpZ0dFrfWtJ1kl5K6f/SxXFv+kYKbC/pTkmvSvqzpLF5+39M0jxJK9K35XcV5CGXts2T9LG8bVtKukHSK5LuAXbqppx/lHRaQdpDkj6uzPmSlqRzPSzp3d1ctp8CR0sq+nyp7G/PW79C0g/ScpOkRZKmp+d7QdKRkj4i6QlJyyT934JTjpB0Tbp290vaLe/cXb4mqWnuWkm/kvQKcFyRvI6WdFU6/hlJ35bUkGrFNwNbp6bSK7q5Hkg6G/gA8LO0/89S+jsl3ZzK9bikTxdcl4sk3SjpNeAASYdJeiC9Di2Svpv3NH9Jf1ek55hS2KQqaT9J90pamf7ul7ctJ+n7xd6Hkkak6/Ryeq/dK2lcd2W2IiLCDz+ICICFwNQi6U3AosL9gD2BZ4HDU3oDMBf4DjAM2BFYAByStn8X+FVanggEMCSt54CngF2ATdL6D9O2XchqDh8GhgLTgfnpOYam5f+b1g8EXgXekY6dCcwCRgLvBp4D7uii/McAd+atTwJWAMOBQ1LZxgAC3gW8rYvz5IB/Bn6SV96pwMK8fQJ4e976FcAP8q53W7qOQ4EvAS8BvwY2B3YFXgd2yLuu64BPpv2/CTydlnvzmqwDjkz7blKkPFcB16fnngg8AZxQ7L3Ri/dODvjnvPWRQAvwRWAIsAewlKy5ruO6rAT2T/kbkc75nrT+XuBF4Mhi76uUdlzHaw5sASwHvpCe7+i0vmUv3odfBv4X2BRoBPYCRlX7/+1Ae7imY/31AeAG4JiImJ3S3gdsFRFnRcTaiFgA/BI4qpfnvDwinoiI18kCxe4p/TPAHyLi5ohYB/wH2QfCfsC+wGZkHwxrI+I2YDZZLaMR+ATwnYh4LSIeAa7s5vl/B+wuafu0/jngtxHRSvbBvDnwTkAR8VhEvNBDef4d+KikXXtZ/nzrgLNTeWcCY4ELI+LViJgHPArslrf/3Ii4Nu3/E7IP533p3WvSHBG/j4j16dpvkK7hUcCZ6bkXAj8m+9AuhcPJgvHlEdEWEQ8A1wGfytvn+oi4M+VvTUTkIuLhtP534DfAh3r5fIcBT0bEf6fn+w3wD+Cjeft09T5cB2xJ9mWhPSLmRsQr/S96fXLQsf46CfhbROTy0rYna2pZ0fEgq4H0tgkif/TTarJgArA18EzHhohYT/bteJu0rSWldXgmbduK7NtsS8G2oiLiVeAPvPGBfDRwddp2G/Az4OfAEkkzJI3qrjAR8VI65qzu9uvCy/HGIIuOQPBi3vbXeeP6QF4Z07VYRHZtevOa5F+fQmPJakz5163j+pbC9sA+Bfn7HJA/KKFT/iTtI2lOau5bSfZeHEvvdHovJYXl6ep9+N/ATcBMSc9LOk/S0F4+ryUOOtZfJwHbSTo/L60FeDoixuQ9No+Ij2zkcz1P9uEEgCQBE8iayp4HJkjKfy9vl7a9RNZMNaFgW3d+Q1ZLmkJWW5jTsSEifhoRe5E1u+0C/Gsv8v4j4ACypph8q8maaTq8aeRXH+WPlGsAtiW7Nr15Tbqban4p2Tf87fPSOq5vfxQ+Vwtwe0H+NouIk7s55tdktewJETEauJisybPYvoU6vZeSXpUnItZFxPciYhJZLftwsiZZ6wMHHSs0NHWYdjy6Gs30KjAN+KCkH6a0e4BXJZ0uaRNJjZLeLel9G5mnWcBhkg5K3yy/AbQCfwPuJvsAny5pqKQmsqaSmamm8Fvgu5I2lTQJOLaH57qR7EPpLOCajhqUpPelb9hDyfqX1gDruz5NJiJWkDVHTS/Y9CDw2XSNptH75qGu7KVswMMQ4Gtk1+cuNvI1SddwFnC2pM1T0+PXgf7+LuhFsn6lDrOBXSR9Ib1+Q9O1flcXx0PWzLksItZI2hv4bN62l8helx2LHpm9vrtI+qykIZI+Q/YlYnYX+28g6QBJ70lNjq+QBeMe3wPWmYOOFbqRrOmm4/HdrnZMH6gfBg6V9P30AXU4WRv402Tfki8BRm9MhiLiceDzwH+mc34U+Gjqo1ib1g9N235B1s/0j3T4aWTNI4vJOqUv7+G5WskC1VSyb9QdRpH1hSwna455mawW0xsXAoW/R/pqyndHc9Lve3murlxP1vfV0Un+8fTNvBSvyVfIAu0C4A6y63JZP/N5IfBJScsl/TQ1aR5M1qT5PNnrdC7Z4I2unAKcJelVsgESszo2RMRq4GzgztRct2/+gRHxMtn1+AbZazidbCDM0l7kfTxwLVnAeQy4nazJDUkXS7q4F+eoe4rwTdzMzKwyXNMxM7OKcdAxM7OKcdAxM7OKcdAxM7OK6evkfoPe2LFjY+LEif069rXXXmPkyJGlzVCNc5nrg8tcHzamzHPnzl0aEVv1tJ+DToGJEydy33339evYXC5HU1NTaTNU41zm+uAy14eNKbOkLmf7yOfmNTMzq5iaCzrKplp/TtKD6fGRvG1nSpqfpj8/JC99WkqbL+mMvPQdJN2d0q+RNKzS5TEzszfUXNBJzo+I3dPjRoA0hclRZNO6TwN+kab0aCSbhPFQsuksjk77QvbL5vMj4u1kv9Q+odIFMTOzN9Rq0CnmCLL5tFoj4mmye6jsnR7zI2JBmhJlJnBEmhTyQLJpKyCb0v7IKuTbzMySWh1IcJqkY4D7gG9ExHKyqcfvyttnEW9MR95SkL4P2X0vVkREW5H9O5F0InAiwLhx48jlcv3K9KpVq/p97EDlMtcHl7k+VKLMVQk6km6h+FTu3wIuAr5PNkX598lm6D2+nPmJiBnADIDJkydHf0dveLRLfXCZ64PLXB5VCToRMbU3+0n6JW9MOf4cne+Lsi1v3AOjWPrLwBhJQ1JtJ39/M7O6NmPuDC69/1LWrl9La1srW43citFrRzN8p+FMmTClbM9bc81rkt6WdxvgfwIeScs3AL+W9BOyu//tTHavEAE7S9qBLKgcBXw2IkLSHLL7xs8ku4/K9ZUriZlZ+TS3NHPenefxwOIHkMSYEWNobWtl+JDhLH99+Ya0juXRw0ezbPUy1rOede3rWLJ6SafzPbb0MQD+fOWfmXPsnLIFnpoLOsB5knYna15bCHwZICLmSZpFdm/4NuDUjtv5SjqN7DayjcBl6R7yAKeT3Vr2B8ADwKWVLIiZWXeaW5q56qGrePSlR3lm5TNvChSFgQRg86Gbs3j1Ypau7s0tgPpubftacgtz9RN0IuIL3Ww7m+wGTYXpN5LdfKwwfQHZ6DYzs7LpTfDYsDx8DKvXraa1vZVnVvbqR/wVNaxxGE0Tm8p2/poLOmZm1TZj7gzOufcc9FA3wSPVQl5f9zoLVy6sdpb7bZvNt2HU8FEb+nTOPPTM+urTMTMrpd7WQgBGDh3JkteWsPT11HS1uooZ76UtNtmCUcNH9dinU7g8rGEYJ+x5AifudeKGc+Vy5WtW6+CgY2YDTv7Iq+5qIWvWreHplU9XO7u9Mn6z8YwYMqLbPp38co4cNpKv7vPVTkFjIHDQMbOa0JsayZp1a1ixZgWLX1tc7ex2qavgUSyQtLa18o6x72D6ftPLXsOoFQ46ZlZWPQ3tBRCqyX6R8ZuNZ/xm4x08SshBx8z6pSOYPP7y4xs+ePObgUYNG8ULq17gpdUvVTurG/S2FvKOse9g6oipnHrEqdXO8qDjoGNmb1IYUBYvX8yIh974sF6zbg0vrn6x2tlkm823YUjDkLLUQupt3rVKcdAxq0OFQSX/w/qFV14oHlBaK5e/njrVi428soHBQcdsEJoxdwYX3HUBr7e9/qY+lNXrVlelyaunob3uF6kPDjpmA1B3I71WrlnJitYVFc3PzlvszJCGIYNqaK+Vh4OOWY0qNupr+evLaV3XyuLVlRky3DF6a/HyxYzYpHNz13ajt2PS2Ekcs9sxrplYrznomFVRVzWWLvtVymDnLXZmbfvabmsn9XhvGSsPBx2zCijWcd/a1lr2Hzl21FQK+1BcS7FqcdAxK6GiwaXMzWHFRnrtPn53d8hbTXLQMeuHYv0t5WoSyx/15ZqKDXQOOmbdyO9zeWn1SwwfMpxnlz7LstuXlfy5CmssHvVlg5GDjlme/N+3DG0YypPLniz5c+R33LvGYvXGQcfqVuH0+K+seYXlrctLcu7CiSIdXMwyDjpWFwqbyVauWcnzq57f6PMW9re4Scysew46Nih1aibTUJ5cvvHNZBPHTGTMiDEsXr6YLUZt4eBi1g8OOjYo5I8me7X1VZat2biO/vzftxTOB+YfSpr1n4OODUj5NZkGGliwYkG/z5U/Pb5/32JWXg46NiB0qsmseZVlrf2vyXQ0k3l6fLPKc9CxmpTf8f/Ey0/0e7qY7prJzKzyHHSsZnQEmrsW3cWDLz7Yr3N0jCZzM5lZbXLQsarq6JtZ/vryftVmOmoybiozGxgcdKziOgLNi6te7PMoM9dkzAY2Bx2riI0JNOM3G88Wm/h3MWaDgYOOlU3HiLO7Ft3V56aziWMmujZjNgjVZNCR9BXgVKAd+ENETE/pZwInpPR/iYibUvo04EKgEbgkIn6Y0ncAZgJbAnOBL0TE2goXp+7MmDuDc/56Ds+sfKbXx4zfbDy7bLmL5yczG+RqLuhIOgA4AtgtIlolvTWlTwKOAnYFtgZukbRLOuznwIeBRcC9km6IiEeBc4HzI2KmpIvJAtZFlS1RfehoPlv86uJeT5o5frPx7Lvtvq7NmNWRmgs6wMnADyOiFSAilqT0I4CZKf1pSfOBvdO2+RGxAEDSTOAISY8BBwKfTftcCXwXB52SaW5p5tuPfJun7nuq181nDjRm9a0Wg84uwAcknQ2sAb4ZEfcC2wB35e23KKUBtBSk70PWpLYiItqK7N+JpBOBEwHGjRtHLpfrV8ZXrVrV72MHknkr5/Gblt9w58t39mr/LYZuwaRRkzhqwlHsOnpXAFqfaiX3VK6MuSyfenmd87nM9aESZa5K0JF0CzC+yKZvkeVpC2Bf4H3ALEk7ljM/ETEDmAEwefLk6O9kjoN9IsjmlmbOuOUM/vLsX3q1/8QxEznz/WcOuhFng/11LsZlrg+VKHNVgk5ETO1qm6STgd9GRAD3SFoPjAWeAybk7bptSqOL9JeBMZKGpNpO/v7WB30JNm4+M7Pu1GLz2u+BA4A5aaDAMGApcAPwa0k/IRtIsDNwDyBg5zRS7TmywQafjYiQNAf4JNkItmOB6ytdmIGsL6PQPrj9B/nhQT90oDGzbtVi0LkMuEzSI8Ba4NhU65knaRbwKNAGnBoR7QCSTgNuIhsyfVlEzEvnOh2YKekHwAPApZUtysDUl2Dz/i3fz3lHnOdgY2a9UnNBJ/2O5vNdbDsbOLtI+o3AjUXSF/DGCDfrQXNLM6f84ZQeJ9vMb0JrfarVAcfMeq3mgo5VXm/7bIoNDBioI9DMrDocdOrcjLkzOGn2SQTR5T6DdRSamVWeg06dam5pZvrN07mj5Y4u99lt3G5cdNhFbj4zs5Jx0KkzvWlKc83GzMrFQaeO9NSUJsS/7v+vnDv13ArnzMzqhYNOnTj9ltM5787zutzu39mYWSU46AxyzS3NnDT7JP6+5O9Ft7spzcwqyUFnEOuuduOmNDOrBgedQerzv/08Vz98ddFtbkozs2px0BlkmluaOf2W0/nrs38tuv1z7/kcv/r4ryqcKzOzjIPOIDJj7gxOnn0y61n/pm3uuzGzWuCgM0h0NxzatRszqxUN1c6AbbzuAs70/ac74JhZzXBNZ4DrKuA0qIGLDrvIzWlmVlMcdAawrgLOpLGTuORjl3h0mpnVHDevDVBd1nBocMAxs5rlms4A1NzSzMmzT+6ySc0Bx8xqlYPOANPc0swxvzvmTcOi3aRmZgOBg84A0tzSzAev+CBt69s6pbtJzcwGCvfpDCDTb57+poAjxEWHu0nNzAYGB50B4vSbT3/TXT6FuPjwiz0s2swGDAedAWDG3Bmc97c3zxbtgGNmA42DTo1rbmnm5D+c/Kb06ftPd8AxswHHQafGnfPXc1gfnUeqTd9/uu+DY2YDkoNODZsxdwazn5zdKe3IdxzpgGNmA5aDTo3qmHEgXwMNTN9/epVyZGa28Rx0alBzSzOn/OGUN8048LF3fMxDo81sQHPQqUHn/e082qO9U9rQhqGu5ZjZgOegU2NmzJ3B7//x+05pk8ZO4vbjbnctx8wGvJoLOpKukfRgeiyU9GDetjMlzZf0uKRD8tKnpbT5ks7IS99B0t0p/RpJwypdnr5obmnm1BtP7ZTWqEZPcWNmg0bNBZ2I+ExE7B4RuwPXAb8FkDQJOArYFZgG/EJSo6RG4OfAocAk4Oi0L8C5wPkR8XZgOXBCZUvTN7mFuU7T3DSogV8c9gsHHDMbNGou6HSQJODTwG9S0hHAzIhojYingfnA3ukxPyIWRMRaYCZwRDr+QODadPyVwJGVLENfvfjai53Wv7nfN/0DUDMbVGp5lukPAC9GxJNpfRvgrrzti1IaQEtB+j7AlsCKiGgrsn8nkk4ETgQYN24cuVyuXxletWpVv4+dt3IeP33wp2/kCbHs+WX9Pl+lbEyZByqXuT64zOVRlaAj6RZgfJFN34qI69Py0bxRyymriJgBzACYPHlyNDU19es8uVyO/h573tXndRoi3aAGjj/g+JpvWtuYMg9ULnN9cJnLoypBJyKmdrdd0hDg48BeecnPARPy1rdNaXSR/jIwRtKQVNvJ37+mzJg7gz/O/2OntI/u8tGaDzhmZn1Vq306U4F/RMSivLQbgKMkDZe0A7AzcA9wL7BzGqk2jGywwQ0REcAc4JPp+GOB66kxXY1Y829yzGwwqtU+naMoaFqLiHmSZgGPAm3AqRHZLyglnQbcBDQCl0XEvHTY6cBMST8AHgAurVD+e80j1sysnnQbdCQ9DAVzseSJiPeWPEfZeY/rIv1s4Owi6TcCNxZJX0A2uq1mLV29tNO6R6yZ2WDWU03n8PS3o/3nv9Pfz5UnO/WluaWZC+6+YMO6EGOGj6lijszMyqvboBMRzwBI+nBE7JG36QxJ9wNnFD/SeuPKh67sdK+cBjXQNLGpehkyMyuz3g4kkKT981b268Ox1oXHlj7Wad0j1sxssOvtQIITgMskjQZENqXM8WXLVR1obmnmjmfu2LDuWaTNrB70KuhExFxgtxR0iIiVZc1VHbjk/ktYT9a0JsQJe5zgWo6ZDXq9aiKTNFrST4BbgVsl/bgjAFnfNbc0c+VDV25YH9Y4jGN2O6aKOTIzq4ze9stcBrxKNgHnp4FXgMvLlanB7qqHrup0k7ZD336oazlmVhd626ezU0R8Im/9e/n3ubG+WbhiYaf18ZsVm4bOzGzw6W1N53VJ7+9YSSPZXi9Plga35pZmbl5w84b1oQ1D3bRmZnWjtzWdk4Er80avLSOby8z6KLcwt6FpzQMIzKze9Hb02oNko9dGpfVXypqrQWx44/ANy0Gwx9v26GZvM7PBpa+j124DbvPotf6b/eTsDcsNNPDy6permBszs8ry6LUKam5p5vaFt29YH9o41NPemFld8ei1Crr8wcs7/SD0i7t/0f05ZlZXPHqtQppbmrniwSs2rPsHoWZWj3pb0zkJuKpg9Npx5crUYJRbmGPd+nWAazlmVr96O3rtITx6baNsuemWG5Y9as3M6lWvgo6k4cAngInAEEkARMRZZcvZILPolUUblj1qzczqVW+b164HVgJzgdbyZWfw6rgtdQMNDB8y3KPWzKwu9TbobBsR08qak0GsuaWZX97/SwAaGhq4YNoF7s8xs7rU29Frf5P0nrLmZBDLLczRtr4NgIhw05qZ1a1uazqSHgYi7fdFSQvImtcERES8t/xZHPhGDR8FZKPWhjUOc9OamdWtnprXDq9ILgax5pZmvvHnbwDQIDetmVl96ynoLI+IVyRtUZHcDEK5hTnWtq/dsO6mNTOrZz0FnV+T1XbmkjWzKW9bADuWKV+DRtPEJiQREQxpGOKmNTOra90GnYg4PP3doTLZGZzWRzbfWhBVzomZWXX1NJBgz+62R8T9pc3O4PPH+X/csNy+vp3cwpz7dMysbvXUvPbjbrYFcGAJ8zIovb4umxfVI9fMzHpuXjugUhnpIGl34GJgBNAGnBIR9yibe+dC4CPAauC4jpqWpGOBb6dT/CAirkzpewFXAJsANwJfjYiKtXE1tzRz4d0XAtDY0OiRa2ZW93p759BNJX1b0oy0vrOkcg2nPg/4XkTsDnwnrQMcCuycHicCF6W8bAH8G7APsDfwb5Leko65CPhS3nEVnVXBPwo1M+ustzMSXA6sBfZL688BPyhLjrJmu1FpeTTwfFo+ArgqMncBYyS9DTgEuDkilkXEcuBmYFraNioi7kq1m6uAI8uU56Lym9I8cs3MrG93Dv2MpKMBImK1OqaaLr2vATdJ+g+yoNgR6LYBWvL2W5TSuktfVCS9YtqjfcOINY9cMzPrfdBZK2kTsloIknZiI2ablnQLML7Ipm8BBwH/JyKuk/Rp4FJgan+fq5f5OZGsyY5x48aRy+X6dZ5Vq1Z1Ovaipy7asNzW3sZlcy6jdbvBNUl3YZnrgctcH1zm8uht0Pk34E/ABElXA/uzEXcOjYgug4ikq4CvptX/AS5Jy88BE/J23TalPQc0FaTnUvq2RfYvlp8ZwAyAyZMnR1NTU7HdepTL5cg/9oHhDzBr0Swa1MDwxuEcf8Dxg24gQWGZ64HLXB9c5vLoVZ9ORNwMfJws0PwGmBwRuTLl6XngQ2n5QODJtHwDcIwy+wIrI+IF4CbgYElvSQMIDgZuSttekbRvago8huy+QBXz7MpnAfj8ez/PrcfcOugCjplZX/V29NpZEfFyRPwhImYDy1KNpxy+BPxY0kPAOaRmL7IhzwuA+cAvgVMAImIZ8H3g3vQ4K6WR9rkkHfMU8MYvNcusuaWZn93zMwD+Z97/VOppzcxqWm+b1yZIOjMi/j3dunoW8EA5MhQRdwB7FUkP4NQujrkMuKxI+n3Au0udx97ILczRFtlw6bXtaz0TgZkZvR8yfTzwHklnAv8L5CLiu2XL1SDQNLEJpX+eicDMLNNt0JG0Z5p/bQ+y2QA+Q9bHcntP87LVuz3etgdBcOAOB7o/x8ws6evca8uBSSndc69147pHrwPgA9t9wAHHzCypubnXBoPmlmaOv+F4AP79jn/n4J0OduAxM6PnWxt8PiJ+JenrxbZHxE/Kk62BLbcwx7r2dQC0rW/zIAIzs6Sn5rWR6e/mRbZ5XpcuNE1sorGhkbb1bR5EYGaWp6fmtf9Kf79XuE3S18qVqYFuyoQpNE1s4v4X7mf20bNdyzEzS3o7ZLqYok1ulnlx1YuMGT6m2tkwM6spGxN0yjXL9IDX3NLMI0seYcGKBRx01UE0tzRXO0tmZjVhY4KO+3S6cNvTt224lUHHbARmZtbz6LVXKR5cRHYLaCvi3W/NZt5poMEDCczM8vQ0kKDYqDXrwVtHvhWAL+z2Bb6815c9kMDMLNmY5jXrwq1P3wrA1B2mOuCYmeVx0Cmx5pZmzrr9LABOnH2iBxGYmeVx0Cmx3MIcbes739LAzMwyDjol1jSxiQZll9WDCMzMOnPQKbEpE6YweevJbLP5Nr6lgZlZAQedMmhb38Z7x73XAcfMrICDThksXb2UrUZuVe1smJnVHAedMli8ajELli3wyDUzswIOOiU25+k5tLa3cmfLnZ53zcysgINOif1p/p8ACMJDps3MCjjolNg7x74TgAZ53jUzs0IOOiW29eZbA/ClPb/kIdNmZgV6ul219dFLq18C4OtTvs4uW+5S5dyYmdUW13RK7KXXsqCz1aYeMm1mVshBp8SWrl5KoxoZM8K3qjYzK+SgU2KPLHmETYZswl2L7qp2VszMao6DTgnNWzmP2U/OZtW6Vf6NjplZETUXdCTtJqlZ0sOS/lfSqLxtZ0qaL+lxSYfkpU9LafMlnZGXvoOku1P6NZKGlTPvD658kPWxHvBtDczMiqm5oANcApwREe8Bfgf8K4CkScBRwK7ANOAXkholNQI/Bw4FJgFHp30BzgXOj4i3A8uBE8qZ8d1H744Q4NsamJkVU4tBZxfgL2n5ZuATafkIYGZEtEbE08B8YO/0mB8RCyJiLTATOEKSgAOBa9PxVwJHljPju47elXEjx7HH+D38Gx0zsyJq8Xc688gCzO+BTwETUvo2QH7v/KKUBtBSkL4PsCWwIiLaiuzfiaQTgRMBxo0bRy6X61fGV61axWutrzFx9ERan2ol91T/zjOQrFq1qt/Xa6BymeuDy1weVQk6km4BxhfZ9C3geOCnkv4fcAOwttz5iYgZwAyAyZMnR1NTU7/OM2fOHFa3r2bSTpPo7zkGmlwuVzdl7eAy1weXuTyqEnQiYmoPuxwMIGkX4LCU9hxv1HoAtk1pdJH+MjBG0pBU28nfvyzWrF9De7Qzevjocj6NmdmAVXN9OpLemv42AN8GLk6bbgCOkjRc0g7AzsA9wL3Azmmk2jBJGUW5AAANy0lEQVSywQY3REQAc4BPpuOPBa4vZ95fa3sNgFHDR/Wwp5lZfaq5oEM2+uwJ4B/A88DlABExD5gFPAr8CTg1ItpTLeY04CbgMWBW2hfgdODrkuaT9fFcWs6MdwSd0SNc0zEzK6bmBhJExIXAhV1sOxs4u0j6jcCNRdIXkI1uq4jX2l3TMTPrTi3WdAasR1c+CkDLypYe9jQzq08OOiXS3NLMfz39XwB87aaveQocM7MiHHRKJLcwR3u0A7CufZ2nwDEzK8JBp0SaJjbRqEYAhjYO9RQ4ZmZFOOiUyJQJUzhsfPaTohs/e6OnwDEzK8JBp4RGD8uGSn9o4oeqnBMzs9rkoFNCre2tbDp0Uxrky2pmVow/HUtozfo1bDp002pnw8ysZjnolNCadgcdM7PuOOiU0Jr1axg5dGS1s2FmVrMcdErINR0zs+456JRQa3srI4e5pmNm1hUHnRLyQAIzs+456JTQmnb36ZiZdcdBp4Rc0zEz656DTgm9tu41Hl/6uGeYNjPrgoNOiTS3NPNq+6vc+/y9HHTVQQ48ZmZFOOiUSMetDIJgbfta39rAzKwIB50S+eD2HwRAiGGNw3xrAzOzIhx0SmTy1pMB+PCOH+bWY271rQ3MzIpw0CmRte1rAfjwTh92wDEz64KDTomsW78OgGGNw6qcEzOz2uWgUyIdNZ2hDUOrnBMzs9rloFMiHUHHNR0zs6456JTIunY3r5mZ9cRBp0Q2NK81unnNzKwrDjol4oEEZmY9c9ApEQ8kMDPrmYNOiXgggZlZz6oSdCR9StI8SeslTS7Ydqak+ZIel3RIXvq0lDZf0hl56TtIujulXyNpWEofntbnp+0Ty1kmDyQwM+tZtWo6jwAfB/6SnyhpEnAUsCswDfiFpEZJjcDPgUOBScDRaV+Ac4HzI+LtwHLghJR+ArA8pZ+f9isbDyQwM+tZVYJORDwWEY8X2XQEMDMiWiPiaWA+sHd6zI+IBRGxFpgJHCFJwIHAten4K4Ej8851ZVq+Fjgo7V8WHkhgZtazIdXOQIFtgLvy1helNICWgvR9gC2BFRHRVmT/bTqOiYg2SSvT/ksLn1TSicCJAOPGjSOXy/U54/cvvR+Ah+5/iNVPru7z8QPVqlWr+nW9BjKXuT64zOVRtqAj6RZgfJFN34qI68v1vP0RETOAGQCTJ0+OpqamPp9jybwlMA/222c/dn3rriXOYe3K5XL053oNZC5zfXCZy6NsQScipvbjsOeACXnr26Y0ukh/GRgjaUiq7eTv33GuRZKGAKPT/mXhgQRmZj2rtSHTNwBHpZFnOwA7A/cA9wI7p5Fqw8gGG9wQEQHMAT6Zjj8WuD7vXMem5U8Ct6X9y8IDCczMelatIdP/JGkRMAX4g6SbACJiHjALeBT4E3BqRLSnWsxpwE3AY8CstC/A6cDXJc0n67O5NKVfCmyZ0r8ObBhmXQ4eSGBm1rOqDCSIiN8Bv+ti29nA2UXSbwRuLJK+gGx0W2H6GuBTG53ZXvKMBGZmPau15rUByzMSmJn1zEGnRDyQwMysZw46JeKBBGZmPXPQKZF169chRKMaq50VM7Oa5aBTImvb1zJEQyjjTDtmZgOeg06JLFyxkCBobmmudlbMzGqWg04JNLc0c92j19EWbRx01UEOPGZmXXDQKYHcwhzt0Q5kzWy5hbnqZsjMrEY56JRA08QmRgwZQQMNDGscRtPEpmpnycysJjnolMCUCVO49ZhbOX6H47n1mFuZMmFKtbNkZlaTau1+OgPWlAlTaN2u1QHHzKwbrumYmVnFOOiYmVnFOOiYmVnFOOiYmVnFOOiYmVnFOOiYmVnFKCKqnYeaIukl4Jl+Hj4WWFrC7AwELnN9cJnrw8aUefuI2KqnnRx0SkjSfRExudr5qCSXuT64zPWhEmV285qZmVWMg46ZmVWMg05pzah2BqrAZa4PLnN9KHuZ3adjZmYV45qOmZlVjIOOmZlVjINOiUiaJulxSfMlnVHt/PSVpMskLZH0SF7aFpJulvRk+vuWlC5JP01l/bukPfOOOTbt/6SkY/PS95L0cDrmp5JU2RJ2JmmCpDmSHpU0T9JXU/pgLvMISfdIeiiV+XspfQdJd6d8XiNpWEofntbnp+0T8851Zkp/XNIheek1+f9AUqOkByTNTuuDusySFqb33oOS7ktptfHejgg/NvIBNAJPATsCw4CHgEnVzlcfy/BBYE/gkby084Az0vIZwLlp+SPAHwEB+wJ3p/QtgAXp71vS8lvStnvSvkrHHlrl8r4N2DMtbw48AUwa5GUWsFlaHgrcnfI3CzgqpV8MnJyWTwEuTstHAdek5UnpPT4c2CG99xtr+f8B8HXg18DstD6oywwsBMYWpNXEe9s1ndLYG5gfEQsiYi0wEziiynnqk4j4C7CsIPkI4Mq0fCVwZF76VZG5Cxgj6W3AIcDNEbEsIpYDNwPT0rZREXFXZO/Yq/LOVRUR8UJE3J+WXwUeA7ZhcJc5ImJVWh2aHgEcCFyb0gvL3HEtrgUOSt9ojwBmRkRrRDwNzCf7P1CT/w8kbQscBlyS1sUgL3MXauK97aBTGtsALXnri1LaQDcuIl5Iy4uBcWm5q/J2l76oSHpNSE0oe5B98x/UZU7NTA8CS8g+RJ4CVkREW9olP58bypa2rwS2pO/XotouAKYD69P6lgz+MgfwZ0lzJZ2Y0mrive3bVVuvRERIGnTj6yVtBlwHfC0iXslvmh6MZY6IdmB3SWOA3wHvrHKWykrS4cCSiJgrqana+amg90fEc5LeCtws6R/5G6v53nZNpzSeAybkrW+b0ga6F1NVmvR3SUrvqrzdpW9bJL2qJA0lCzhXR8RvU/KgLnOHiFgBzAGmkDWndHwBzc/nhrKl7aOBl+n7taim/YGPSVpI1vR1IHAhg7vMRMRz6e8Ssi8Xe1Mr7+1qd3gNhgdZjXEBWQdjR2firtXOVz/KMZHOAwl+ROeOx/PS8mF07ni8J6VvATxN1un4lrS8RdpW2PH4kSqXVWRt0RcUpA/mMm8FjEnLmwB/BQ4H/ofOneqnpOVT6dypPist70rnTvUFZB3qNf3/AGjijYEEg7bMwEhg87zlvwHTauW9XfU3wmB5kI0AeYKsjfxb1c5PP/L/G+AFYB1ZG+0JZG3ZtwJPArfkveEE/DyV9WFgct55jifrZJ0PfDEvfTLwSDrmZ6TZMKpY3veTtXv/HXgwPT4yyMv8XuCBVOZHgO+k9B3Th8j89GE8PKWPSOvz0/Yd8871rVSux8kbuVTL/w/oHHQGbZlT2R5Kj3kdeaqV97anwTEzs4pxn46ZmVWMg46ZmVWMg46ZmVWMg46ZmVWMg46ZmVWMg44NepJC0o/z1r8p6bslOvcVkj5ZinP18DyfkvSYpDkF6VtLujYt7y7pIyV8zjGSTin2XGb95aBj9aAV+LiksdXOSL68X8T3xgnAlyLigPzEiHg+IjqC3u5kvxkpVR7GkM26XOy5zPrFQcfqQRvZvd//T+GGwpqKpFXpb5Ok2yVdL2mBpB9K+pyy+9E8LGmnvNNMlXSfpCfSXF8dE2v+SNK96R4lX847718l3QA8WiQ/R6fzPyLp3JT2HbIfs14q6UcF+09M+w4DzgI+k+6h8hlJI5XdJ+keZfeSOSIdc5ykGyTdBtwqaTNJt0q6Pz13xyzJPwR2Suf7UcdzpXOMkHR52v8BSQfknfu3kv6U7sFyXt71uCLl9WFJb3otrD54wk+rFz8H/t7xIdhLuwHvIrvlwwLgkojYW9kN374CfC3tN5FsbqudgDmS3g4cA6yMiPdJGg7cKenPaf89gXdHNkX+BpK2Bs4F9gKWk80SfGREnCXpQOCbEXFfsYxGxNoUnCZHxGnpfOcAt0XE8WmCz3sk3ZKXh/dGxLJU2/mnyCY8HQvclYLiGSmfu6fzTcx7ylOzp433SHpnyusuadvuZLN2twKPS/pP4K3ANhHx7nSuMT1cexukXNOxuhARr5DNtfYvfTjs3sjuu9NKNt1HR9B4mCzQdJgVEesj4kmy4PRO4GDgGGW3EbibbAqSndP+9xQGnOR9QC4iXopsWv2ryW6u118HA2ekPOTIpnjZLm27OSI67p8k4BxJfyebHmUb3pj2vivvB34FEBH/AJ4BOoLOrRGxMiLWkNXmtie7LjtK+k9J04BXNqJcNoC5pmP15ALgfuDyvLQ20pcvSQ1kkzZ2aM1bXp+3vp7O/3cK55IKsg/yr0TETfkblE2v/1r/st9nAj4REY8X5GGfgjx8jmwy0L0iYl2akXnERjxv/nVrB4ZExHJJu5HdGOwk4NNk83pZnXFNx+pG+mY/i6xTvsNCsuYsgI+R3U2zrz4lqSH18+xINiHkTcDJym6fgKRdJI3s4Tz3AB+SNFZSI3A0cHsf8vEq2a23O9wEfEXKbhIkaY8ujhtNds+ZdalvZvsuzpfvr2TBitSsth1ZuYtKzXYNEXEd8G2y5j2rQw46Vm9+DOSPYvsl2Qf9Q2T3lulPLeRZsoDxR+Ck1Kx0CVnT0v2p8/2/6KFlIbK7Op5Bdp+bh4C5EXF9H/IxB5jUMZAA+D5ZEP27pHlpvZirgcmSHibri/pHys/LZH1RjxQOYAB+ATSkY64BjkvNkF3ZBsilpr5fAWf2oVw2iHiWaTMzqxjXdMzMrGIcdMzMrGIcdMzMrGIcdMzMrGIcdMzMrGIcdMzMrGIcdMzMrGL+P2oOZrH7iF9AAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell to plot Likelihood v/s Number of Iterations.\n",
    "iters = np.array(range(0,num_iters,100))\n",
    "plt.plot(iters,likelihood_values,'.-',color='green')\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Likelihood')\n",
    "plt.title(\"Likelihood vs Number of Iterations.\")\n",
    "plt.grid()\n",
    "print(likelihood_values[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data = 0.900133\n",
      "[[-1.34643759e-06 -1.34643759e-06  1.56712564e+00 ...  5.73349422e-01\n",
      "  -1.34643759e-06  7.44745727e-01]]\n",
      "[-1.34643759]\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(C=100000000)\n",
    "logreg.fit(X_tr_scale, Y_tr.ravel())\n",
    "# TODO - Find the predicted values on training set using logreg.predict - 5 points\n",
    "yhat = logreg.predict(X_tr_scale)\n",
    "# TODO - Find the accuracy achieved on training set using logreg.score - 5 points\n",
    "acc = logreg.score(X_tr_scale, Y_tr.ravel())\n",
    "\n",
    "print(\"Accuracy on training data = %f\" % acc)\n",
    "\n",
    "w = logreg.coef_\n",
    "intercept = logreg.intercept_\n",
    "# VERIFY - Compare the parameters computed by logreg model and gradient ascent. They should be nearly same.\n",
    "print(w)\n",
    "print(intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names =['id', 'comment_text']\n",
    "df = pd.read_csv('test.csv',header=None,sep=\",\",names=names, encoding = \"ISO-8859-1\")\n",
    "df = df.drop(['id'], axis=1)\n",
    "df.drop(df.index[:150000], inplace=True)\n",
    "\n",
    "X_te = data[:,0].reshape((df.shape[0],1))\n",
    "Y_te = np.zeros((df.shape[0], 1))\n",
    "X_test = []\n",
    "for index in range(0,df.shape[0]):\n",
    "    X_test.append(X_te[index][0])\n",
    "    \n",
    "cv = sklearn.feature_extraction.text.CountVectorizer(vocabulary=bad_words)\n",
    "X_test_features = cv.fit_transform(X_test).toarray()\n",
    "\n",
    "X_scale = StandardScaler()\n",
    "X_tr = X_scale.fit_transform(X_tr_features)\n",
    "X_te = X_scale.fit_transform(X_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up the output layer\n",
    "def convert_y_to_vect(y):\n",
    "    y_vect = np.zeros((len(y), 10))\n",
    "    for i in range(len(y)):\n",
    "        y_vect[i, y[i]] = 1\n",
    "    return y_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the training and test targets to vectors\n",
    "y_v_train = convert_y_to_vect(y_train)\n",
    "y_v_test = convert_y_to_vect(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A quick check to see that our code performs as we expect \n",
    "print(y_train[0:4])\n",
    "print(y_v_train[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def f_deriv(z):\n",
    "    return f(z) * (1 - f(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_and_init_weights(nn_structure):\n",
    "    W = {} #creating a dictionary i.e. a set of key: value pairs\n",
    "    b = {}\n",
    "    for l in range(1, len(nn_structure)):\n",
    "        W[l] = r.random_sample((nn_structure[l], nn_structure[l-1])) #Return “continuous uniform” random floats in the half-open interval [0.0, 1.0). \n",
    "        b[l] = r.random_sample((nn_structure[l],))\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_tri_values(nn_structure):\n",
    "    tri_W = {}\n",
    "    tri_b = {}\n",
    "    for l in range(1, len(nn_structure)):\n",
    "        tri_W[l] = np.zeros((nn_structure[l], nn_structure[l-1]))\n",
    "        tri_b[l] = np.zeros((nn_structure[l],))\n",
    "    return tri_W, tri_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(x, W, b):\n",
    "    a = {1: x} # create a dictionary for holding the a values for all levels\n",
    "    z = { } # create a dictionary for holding the z values for all the layers\n",
    "    for l in range(1, len(W) + 1): # for each layer\n",
    "        node_in = a[l]\n",
    "        z[l+1] = W[l].dot(node_in) + b[l]  # z^(l+1) = W^(l)*a^(l) + b^(l)\n",
    "        a[l+1] = f(z[l+1]) # a^(l+1) = f(z^(l+1))\n",
    "    return a, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_out_layer_delta(y, a_out, z_out):\n",
    "    # delta^(nl) = -(y_i - a_i^(nl)) * f'(z_i^(nl))\n",
    "    return -(y-a_out) * f_deriv(z_out) \n",
    "\n",
    "\n",
    "def calculate_hidden_delta(delta_plus_1, w_l, z_l):\n",
    "    # delta^(l) = (transpose(W^(l)) * delta^(l+1)) * f'(z^(l))\n",
    "    return np.dot(np.transpose(w_l), delta_plus_1) * f_deriv(z_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Back Propagation Algorithm\n",
    "def train_nn(nn_structure, X, y, iter_num=3000, alpha=0.25):\n",
    "    W, b = setup_and_init_weights(nn_structure)\n",
    "    cnt = 0\n",
    "    N = len(y)\n",
    "    avg_cost_func = []\n",
    "    print('Starting gradient descent for {} iterations'.format(iter_num))\n",
    "    while cnt < iter_num:\n",
    "        if cnt%1000 == 0:\n",
    "            print('Iteration {} of {}'.format(cnt, iter_num))\n",
    "        tri_W, tri_b = init_tri_values(nn_structure)\n",
    "        avg_cost = 0\n",
    "        for i in range(N):\n",
    "            delta = {}\n",
    "            # perform the feed forward pass and return the stored a and z values, to be used in the\n",
    "            # gradient descent step\n",
    "            a, z = feed_forward(X[i, :], W, b)\n",
    "            # loop from nl-1 to 1 backpropagating the errors\n",
    "            for l in range(len(nn_structure), 0, -1):\n",
    "                if l == len(nn_structure):\n",
    "                    delta[l] = calculate_out_layer_delta(y[i,:], a[l], z[l])\n",
    "                    avg_cost += np.linalg.norm((y[i,:]-a[l]))\n",
    "                else:\n",
    "                    if l > 1:\n",
    "                        delta[l] = calculate_hidden_delta(delta[l+1], W[l], z[l])\n",
    "                    # triW^(l) = triW^(l) + delta^(l+1) * transpose(a^(l))\n",
    "                    tri_W[l] += np.dot(delta[l+1][:,np.newaxis], np.transpose(a[l][:,np.newaxis]))# np.newaxis increase the number of dimensions\n",
    "                    # trib^(l) = trib^(l) + delta^(l+1)\n",
    "                    tri_b[l] += delta[l+1]\n",
    "        # perform the gradient descent step for the weights in each layer\n",
    "        for l in range(len(nn_structure) - 1, 0, -1):\n",
    "            W[l] += -alpha * (1.0/N * tri_W[l])\n",
    "            b[l] += -alpha * (1.0/N * tri_b[l])\n",
    "        # complete the average cost calculation\n",
    "        avg_cost = 1.0/N * avg_cost\n",
    "        avg_cost_func.append(avg_cost)\n",
    "        cnt += 1\n",
    "    return W, b, avg_cost_func\n",
    "\n",
    "\n",
    "def predict_y(W, b, X, n_layers):\n",
    "    N = X.shape[0]\n",
    "    y = np.zeros((N,))\n",
    "    for i in range(N):\n",
    "        a, z = feed_forward(X[i, :], W, b)\n",
    "        y[i] = np.argmax(a[n_layers])\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_structure = [64, 30, 10]\n",
    "    \n",
    "# train the NN\n",
    "W, b, avg_cost_func = train_nn(nn_structure, X_train, y_v_train, 3000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
